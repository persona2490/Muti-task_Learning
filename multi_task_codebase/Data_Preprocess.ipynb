{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        rating                                       title  \\\n",
      "558261     5.0                                  Neon pink.   \n",
      "533565     5.0                             Godd experience   \n",
      "275352     5.0                Love This..made my mom's day   \n",
      "658564     1.0                                      Review   \n",
      "48163      5.0  Battery will not stay charged (Rescinding)   \n",
      "\n",
      "                                                     text      timestamp  \\\n",
      "558261  Nice formula, smooth application with no pooli...  1431209081000   \n",
      "533565             Great product. It came when  expected.  1496279099000   \n",
      "275352        My 90 year old mother's favorite fragrance.  1561392844340   \n",
      "658564                                  Just not worth it  1497560837278   \n",
      "48163   The Battery life on this product has always be...  1580137117959   \n",
      "\n",
      "        verified_purchase        task  \n",
      "558261               True  All_Beauty  \n",
      "533565               True  All_Beauty  \n",
      "275352               True  All_Beauty  \n",
      "658564               True  All_Beauty  \n",
      "48163                True  All_Beauty  \n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# # Step 1: 文件路径\n",
    "# file_path = \"../datasets/All_Beauty.jsonl\"  # 你需要确保文件路径正确\n",
    "\n",
    "# # Step 2: 读取文件并将其存储为 DataFrame\n",
    "# def load_jsonl_to_dataframe(file_path, sample_size=60000):\n",
    "#     data = []\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         for line in file:\n",
    "#             data.append(json.loads(line.strip()))\n",
    "#     df = pd.DataFrame(data)\n",
    "#     # Step 3: 随机抽取 sample_size 条数据\n",
    "#     return df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# # 读取并抽取 60000 条记录\n",
    "# df_all_beauty = load_jsonl_to_dataframe(file_path)\n",
    "\n",
    "# # Step 4: 删除不必要的字段\n",
    "# # 保留的字段: 'rating', 'title', 'text', 'timestamp', 'verified_purchase', 'helpful_votes'\n",
    "# def clean_dataframe(df):\n",
    "#     fields_to_keep = ['rating', 'title', 'text', 'timestamp', 'verified_purchase']\n",
    "#     return df[fields_to_keep].dropna()  # 去除缺失值\n",
    "\n",
    "# df_all_beauty_clean = clean_dataframe(df_all_beauty)\n",
    "\n",
    "# # Step 5: 添加任务标识符\n",
    "# df_all_beauty_clean['task'] = 'All_Beauty'\n",
    "\n",
    "# # Step 6: 保存处理后的数据到CSV文件\n",
    "# output_csv = \"all_beauty_60000.csv\"\n",
    "# df_all_beauty_clean.to_csv(output_csv, index=False)\n",
    "\n",
    "# # 查看处理后的数据集的前几行\n",
    "# print(df_all_beauty_clean.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         rating                       title  \\\n",
      "1947071     5.0                 Great Scott   \n",
      "4194565     5.0             Still way ahead   \n",
      "4293526     4.0                Entertaining   \n",
      "4308226     4.0  I love it great background   \n",
      "4409775     3.0       Voices Drown by Music   \n",
      "\n",
      "                                                      text      timestamp  \\\n",
      "1947071  Ok...am I missing something here? Scott Walker...   995566729000   \n",
      "4194565  \"When I was a little kid in La Jolla, Californ...  1157278476000   \n",
      "4293526  I generally dont get into screamo, but i would...  1156845285000   \n",
      "4308226                  I love it  great background music  1435118556000   \n",
      "4409775  The Alan Parsons Symphonic Project Live in Col...  1677527967251   \n",
      "\n",
      "         verified_purchase           task  \n",
      "1947071              False  CDs_and_Vinyl  \n",
      "4194565              False  CDs_and_Vinyl  \n",
      "4293526              False  CDs_and_Vinyl  \n",
      "4308226               True  CDs_and_Vinyl  \n",
      "4409775              False  CDs_and_Vinyl  \n"
     ]
    }
   ],
   "source": [
    "# # 处理第二个数据集 CDs_and_Vinyl\n",
    "# file_path = \"../datasets/CDs_and_Vinyl.jsonl\"\n",
    "\n",
    "# # 读取并抽取 60000 条记录\n",
    "# df_cds_and_vinyl = load_jsonl_to_dataframe(file_path)\n",
    "\n",
    "# # 删除不必要的字段\n",
    "# df_cds_and_vinyl_clean = clean_dataframe(df_cds_and_vinyl)\n",
    "\n",
    "# # 添加任务标识符\n",
    "# df_cds_and_vinyl_clean['task'] = 'CDs_and_Vinyl'\n",
    "\n",
    "# # 保存到 CSV 文件\n",
    "# output_csv = \"cds_and_vinyl_60000.csv\"\n",
    "# df_cds_and_vinyl_clean.to_csv(output_csv, index=False)\n",
    "\n",
    "# # 查看处理后的数据集\n",
    "# print(df_cds_and_vinyl_clean.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         rating                                              title  \\\n",
      "1112677     5.0                        What can I say?  ZELDA=Good   \n",
      "612761      5.0                            2K brings the heat back   \n",
      "1402617     5.0  Awesome kit that requires some patience to ass...   \n",
      "3777127     5.0          Beautiful Graphics and Addicting Gameplay   \n",
      "1549429     3.0  There's something not right about this install...   \n",
      "\n",
      "                                                      text      timestamp  \\\n",
      "1112677  The origional LOZ was and still is my favorite...  1050036869000   \n",
      "612761   2K has brought it back with this edition. I ha...  1538079506184   \n",
      "1402617  It is very important to note that this device ...  1531763857729   \n",
      "3777127  Let me be the first to say that this is not a ...  1265755938000   \n",
      "1549429  I know what it is now, IT'S ALMOST IMPOSSIBLE ...  1035781744000   \n",
      "\n",
      "         verified_purchase         task  \n",
      "1112677              False  Video_Games  \n",
      "612761               False  Video_Games  \n",
      "1402617               True  Video_Games  \n",
      "3777127               True  Video_Games  \n",
      "1549429              False  Video_Games  \n"
     ]
    }
   ],
   "source": [
    "# # 处理第三个数据集 Video_Games\n",
    "# file_path = \"../datasets/Video_Games.jsonl\"\n",
    "\n",
    "# # 读取并抽取 60000 条记录\n",
    "# df_video_games = load_jsonl_to_dataframe(file_path)\n",
    "\n",
    "# # 删除不必要的字段\n",
    "# df_video_games_clean = clean_dataframe(df_video_games)\n",
    "\n",
    "# # 添加任务标识符\n",
    "# df_video_games_clean['task'] = 'Video_Games'\n",
    "\n",
    "# # 保存到 CSV 文件\n",
    "# output_csv = \"video_games_60000.csv\"\n",
    "# df_video_games_clean.to_csv(output_csv, index=False)\n",
    "\n",
    "# # 查看处理后的数据集\n",
    "# print(df_video_games_clean.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating                                       title  \\\n",
      "0     5.0                                  Neon pink.   \n",
      "1     5.0                             Godd experience   \n",
      "2     5.0                Love This..made my mom's day   \n",
      "3     1.0                                      Review   \n",
      "4     5.0  Battery will not stay charged (Rescinding)   \n",
      "\n",
      "                                                text      timestamp  \\\n",
      "0  Nice formula, smooth application with no pooli...  1431209081000   \n",
      "1             Great product. It came when  expected.  1496279099000   \n",
      "2        My 90 year old mother's favorite fragrance.  1561392844340   \n",
      "3                                  Just not worth it  1497560837278   \n",
      "4  The Battery life on this product has always be...  1580137117959   \n",
      "\n",
      "   verified_purchase        task  \n",
      "0               True  All_Beauty  \n",
      "1               True  All_Beauty  \n",
      "2               True  All_Beauty  \n",
      "3               True  All_Beauty  \n",
      "4               True  All_Beauty  \n"
     ]
    }
   ],
   "source": [
    "# # 读取每个处理后的 CSV 文件\n",
    "# df_all_beauty_clean = pd.read_csv(\"all_beauty_60000.csv\")\n",
    "# df_cds_and_vinyl_clean = pd.read_csv(\"cds_and_vinyl_60000.csv\")\n",
    "# df_video_games_clean = pd.read_csv(\"video_games_60000.csv\")\n",
    "\n",
    "# # 合并所有数据集\n",
    "# df_merged = pd.concat([df_all_beauty_clean, df_cds_and_vinyl_clean, df_video_games_clean], ignore_index=True)\n",
    "\n",
    "# # 保存最终的合并数据集到 CSV 文件\n",
    "# df_merged.to_csv(\"merged_amazon_reviews_180000.csv\", index=False)\n",
    "\n",
    "# # 查看合并后的数据集\n",
    "# print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_purchase\n",
      "True     146595\n",
      "False     33405\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_merged = pd.read_csv(\"merged_amazon_reviews_180000.csv\")\n",
    "\n",
    "# 统计 verified_purchase 为 True 和 False 的数量\n",
    "verified_purchase_counts = df_merged['verified_purchase'].value_counts()\n",
    "\n",
    "# 打印统计结果\n",
    "print(verified_purchase_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     c:\\Users\\swang\\anaconda3\\envs\\nlp\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     c:\\Users\\swang\\anaconda3\\envs\\nlp\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     c:\\Users\\swang\\anaconda3\\envs\\nlp\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# 下载 nltk 的必要资源（如果没有）\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 初始化词形还原工具\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 停用词列表\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新的文本预处理函数，加入类型检查\n",
    "def preprocess_text(text):\n",
    "    # 先检查是否是字符串类型，如果不是则转换为字符串\n",
    "    if isinstance(text, str):\n",
    "        # 将文本转换为小写\n",
    "        text = text.lower()\n",
    "\n",
    "        # 移除方括号内的内容\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "        # 移除特殊字符和标点符号\n",
    "        text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "\n",
    "        # 移除换行符、回车符等\n",
    "        text = re.sub(r'\\n', '', text)\n",
    "\n",
    "        # 移除数字\n",
    "        text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    # 如果不是字符串类型，可以返回空字符串或原值\n",
    "    else:\n",
    "        text = str(text)  # 将其转换为字符串，或你可以选择直接返回空字符串 ''\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 应用到数据中的 'text' 列\n",
    "df_merged['cleaned_text'] = df_merged['text'].apply(preprocess_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text\n",
      "0  nice formula smooth application with no poolin...\n",
      "1               great product it came when  expected\n",
      "2            my  year old mothers favorite fragrance\n",
      "3                                  just not worth it\n",
      "4  the battery life on this product has always be...\n"
     ]
    }
   ],
   "source": [
    "print(df_merged[['cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords:  179\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Number of stopwords: \", len(stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\swang/nltk_data', 'c:\\\\Users\\\\swang\\\\anaconda3\\\\envs\\\\nlp\\\\nltk_data', 'c:\\\\Users\\\\swang\\\\anaconda3\\\\envs\\\\nlp\\\\share\\\\nltk_data', 'c:\\\\Users\\\\swang\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\swang\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "print(df_merged['cleaned_text'].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\swang/nltk_data'\n    - 'c:\\\\Users\\\\swang\\\\anaconda3\\\\envs\\\\nlp\\\\nltk_data'\n    - 'c:\\\\Users\\\\swang\\\\anaconda3\\\\envs\\\\nlp\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\swang\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\swang\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 应用到 'cleaned_text' 列\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m df_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_merged\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_stopwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 查看处理后的数据\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_merged[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mremove_stopwords\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_stopwords\u001b[39m(text):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):  \n\u001b[1;32m----> 7\u001b[0m         words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# 删除停用词\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\swang/nltk_data'\n    - 'c:\\\\Users\\\\swang\\\\anaconda3\\\\envs\\\\nlp\\\\nltk_data'\n    - 'c:\\\\Users\\\\swang\\\\anaconda3\\\\envs\\\\nlp\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\swang\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\swang\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Step 3: 删除停用词，加入类型检查\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):  \n",
    "    \n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        # 删除停用词\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        \n",
    "        return ' '.join(filtered_words)\n",
    "    else:\n",
    "        # 如果不是字符串类型，返回原始值\n",
    "        return text\n",
    "\n",
    "# 应用到 'cleaned_text' 列\n",
    "df_merged['cleaned_text'] = df_merged['cleaned_text'].apply(remove_stopwords)\n",
    "\n",
    "# 查看处理后的数据\n",
    "print(df_merged[['text', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
