{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        rating                                       title  \\\n",
      "558261     5.0                                  Neon pink.   \n",
      "533565     5.0                             Godd experience   \n",
      "275352     5.0                Love This..made my mom's day   \n",
      "658564     1.0                                      Review   \n",
      "48163      5.0  Battery will not stay charged (Rescinding)   \n",
      "\n",
      "                                                     text      timestamp  \\\n",
      "558261  Nice formula, smooth application with no pooli...  1431209081000   \n",
      "533565             Great product. It came when  expected.  1496279099000   \n",
      "275352        My 90 year old mother's favorite fragrance.  1561392844340   \n",
      "658564                                  Just not worth it  1497560837278   \n",
      "48163   The Battery life on this product has always be...  1580137117959   \n",
      "\n",
      "        verified_purchase        task  \n",
      "558261               True  All_Beauty  \n",
      "533565               True  All_Beauty  \n",
      "275352               True  All_Beauty  \n",
      "658564               True  All_Beauty  \n",
      "48163                True  All_Beauty  \n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# # Step 1: 文件路径\n",
    "# file_path = \"../datasets/All_Beauty.jsonl\"  # 你需要确保文件路径正确\n",
    "\n",
    "# # Step 2: 读取文件并将其存储为 DataFrame\n",
    "# def load_jsonl_to_dataframe(file_path, sample_size=60000):\n",
    "#     data = []\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         for line in file:\n",
    "#             data.append(json.loads(line.strip()))\n",
    "#     df = pd.DataFrame(data)\n",
    "#     # Step 3: 随机抽取 sample_size 条数据\n",
    "#     return df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# # 读取并抽取 60000 条记录\n",
    "# df_all_beauty = load_jsonl_to_dataframe(file_path)\n",
    "\n",
    "# # Step 4: 删除不必要的字段\n",
    "# # 保留的字段: 'rating', 'title', 'text', 'timestamp', 'verified_purchase', 'helpful_votes'\n",
    "# def clean_dataframe(df):\n",
    "#     fields_to_keep = ['rating', 'title', 'text', 'timestamp', 'verified_purchase']\n",
    "#     return df[fields_to_keep].dropna()  # 去除缺失值\n",
    "\n",
    "# df_all_beauty_clean = clean_dataframe(df_all_beauty)\n",
    "\n",
    "# # Step 5: 添加任务标识符\n",
    "# df_all_beauty_clean['task'] = 'All_Beauty'\n",
    "\n",
    "# # Step 6: 保存处理后的数据到CSV文件\n",
    "# output_csv = \"all_beauty_60000.csv\"\n",
    "# df_all_beauty_clean.to_csv(output_csv, index=False)\n",
    "\n",
    "# # 查看处理后的数据集的前几行\n",
    "# print(df_all_beauty_clean.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         rating                       title  \\\n",
      "1947071     5.0                 Great Scott   \n",
      "4194565     5.0             Still way ahead   \n",
      "4293526     4.0                Entertaining   \n",
      "4308226     4.0  I love it great background   \n",
      "4409775     3.0       Voices Drown by Music   \n",
      "\n",
      "                                                      text      timestamp  \\\n",
      "1947071  Ok...am I missing something here? Scott Walker...   995566729000   \n",
      "4194565  \"When I was a little kid in La Jolla, Californ...  1157278476000   \n",
      "4293526  I generally dont get into screamo, but i would...  1156845285000   \n",
      "4308226                  I love it  great background music  1435118556000   \n",
      "4409775  The Alan Parsons Symphonic Project Live in Col...  1677527967251   \n",
      "\n",
      "         verified_purchase           task  \n",
      "1947071              False  CDs_and_Vinyl  \n",
      "4194565              False  CDs_and_Vinyl  \n",
      "4293526              False  CDs_and_Vinyl  \n",
      "4308226               True  CDs_and_Vinyl  \n",
      "4409775              False  CDs_and_Vinyl  \n"
     ]
    }
   ],
   "source": [
    "# # 处理第二个数据集 CDs_and_Vinyl\n",
    "# file_path = \"../datasets/CDs_and_Vinyl.jsonl\"\n",
    "\n",
    "# # 读取并抽取 60000 条记录\n",
    "# df_cds_and_vinyl = load_jsonl_to_dataframe(file_path)\n",
    "\n",
    "# # 删除不必要的字段\n",
    "# df_cds_and_vinyl_clean = clean_dataframe(df_cds_and_vinyl)\n",
    "\n",
    "# # 添加任务标识符\n",
    "# df_cds_and_vinyl_clean['task'] = 'CDs_and_Vinyl'\n",
    "\n",
    "# # 保存到 CSV 文件\n",
    "# output_csv = \"cds_and_vinyl_60000.csv\"\n",
    "# df_cds_and_vinyl_clean.to_csv(output_csv, index=False)\n",
    "\n",
    "# # 查看处理后的数据集\n",
    "# print(df_cds_and_vinyl_clean.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         rating                                              title  \\\n",
      "1112677     5.0                        What can I say?  ZELDA=Good   \n",
      "612761      5.0                            2K brings the heat back   \n",
      "1402617     5.0  Awesome kit that requires some patience to ass...   \n",
      "3777127     5.0          Beautiful Graphics and Addicting Gameplay   \n",
      "1549429     3.0  There's something not right about this install...   \n",
      "\n",
      "                                                      text      timestamp  \\\n",
      "1112677  The origional LOZ was and still is my favorite...  1050036869000   \n",
      "612761   2K has brought it back with this edition. I ha...  1538079506184   \n",
      "1402617  It is very important to note that this device ...  1531763857729   \n",
      "3777127  Let me be the first to say that this is not a ...  1265755938000   \n",
      "1549429  I know what it is now, IT'S ALMOST IMPOSSIBLE ...  1035781744000   \n",
      "\n",
      "         verified_purchase         task  \n",
      "1112677              False  Video_Games  \n",
      "612761               False  Video_Games  \n",
      "1402617               True  Video_Games  \n",
      "3777127               True  Video_Games  \n",
      "1549429              False  Video_Games  \n"
     ]
    }
   ],
   "source": [
    "# # 处理第三个数据集 Video_Games\n",
    "# file_path = \"../datasets/Video_Games.jsonl\"\n",
    "\n",
    "# # 读取并抽取 60000 条记录\n",
    "# df_video_games = load_jsonl_to_dataframe(file_path)\n",
    "\n",
    "# # 删除不必要的字段\n",
    "# df_video_games_clean = clean_dataframe(df_video_games)\n",
    "\n",
    "# # 添加任务标识符\n",
    "# df_video_games_clean['task'] = 'Video_Games'\n",
    "\n",
    "# # 保存到 CSV 文件\n",
    "# output_csv = \"video_games_60000.csv\"\n",
    "# df_video_games_clean.to_csv(output_csv, index=False)\n",
    "\n",
    "# # 查看处理后的数据集\n",
    "# print(df_video_games_clean.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating                                       title  \\\n",
      "0     5.0                                  Neon pink.   \n",
      "1     5.0                             Godd experience   \n",
      "2     5.0                Love This..made my mom's day   \n",
      "3     1.0                                      Review   \n",
      "4     5.0  Battery will not stay charged (Rescinding)   \n",
      "\n",
      "                                                text      timestamp  \\\n",
      "0  Nice formula, smooth application with no pooli...  1431209081000   \n",
      "1             Great product. It came when  expected.  1496279099000   \n",
      "2        My 90 year old mother's favorite fragrance.  1561392844340   \n",
      "3                                  Just not worth it  1497560837278   \n",
      "4  The Battery life on this product has always be...  1580137117959   \n",
      "\n",
      "   verified_purchase        task  \n",
      "0               True  All_Beauty  \n",
      "1               True  All_Beauty  \n",
      "2               True  All_Beauty  \n",
      "3               True  All_Beauty  \n",
      "4               True  All_Beauty  \n"
     ]
    }
   ],
   "source": [
    "# # 读取每个处理后的 CSV 文件\n",
    "# df_all_beauty_clean = pd.read_csv(\"all_beauty_60000.csv\")\n",
    "# df_cds_and_vinyl_clean = pd.read_csv(\"cds_and_vinyl_60000.csv\")\n",
    "# df_video_games_clean = pd.read_csv(\"video_games_60000.csv\")\n",
    "\n",
    "# # 合并所有数据集\n",
    "# df_merged = pd.concat([df_all_beauty_clean, df_cds_and_vinyl_clean, df_video_games_clean], ignore_index=True)\n",
    "\n",
    "# # 保存最终的合并数据集到 CSV 文件\n",
    "# df_merged.to_csv(\"merged_amazon_reviews_180000.csv\", index=False)\n",
    "\n",
    "# # 查看合并后的数据集\n",
    "# print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_purchase\n",
      "True     146595\n",
      "False     33405\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_merged = pd.read_csv(\"merged_amazon_reviews_180000.csv\")\n",
    "\n",
    "# 统计 verified_purchase 为 True 和 False 的数量\n",
    "verified_purchase_counts = df_merged['verified_purchase'].value_counts()\n",
    "\n",
    "# 打印统计结果\n",
    "print(verified_purchase_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     c:\\Users\\swang\\anaconda3\\envs\\nlp\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     c:\\Users\\swang\\anaconda3\\envs\\nlp\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     c:\\Users\\swang\\anaconda3\\envs\\nlp\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# 下载 nltk 的必要资源（如果没有）\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 初始化词形还原工具\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 停用词列表\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新的文本预处理函数，加入类型检查\n",
    "def preprocess_text(text):\n",
    "    # 先检查是否是字符串类型，如果不是则转换为字符串\n",
    "    if isinstance(text, str):\n",
    "        # 将文本转换为小写\n",
    "        text = text.lower()\n",
    "\n",
    "        # 移除方括号内的内容\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "        # 移除特殊字符和标点符号\n",
    "        text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "\n",
    "        # 移除换行符、回车符等\n",
    "        text = re.sub(r'\\n', '', text)\n",
    "\n",
    "        # 移除数字\n",
    "        text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    # 如果不是字符串类型，可以返回空字符串或原值\n",
    "    else:\n",
    "        text = str(text)  # 将其转换为字符串，或你可以选择直接返回空字符串 ''\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 应用到数据中的 'text' 列\n",
    "df_merged['cleaned_text'] = df_merged['text'].apply(preprocess_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text\n",
      "0  nice formula smooth application with no poolin...\n",
      "1               great product it came when  expected\n",
      "2            my  year old mothers favorite fragrance\n",
      "3                                  just not worth it\n",
      "4  the battery life on this product has always be...\n"
     ]
    }
   ],
   "source": [
    "print(df_merged[['cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords:  179\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Number of stopwords: \", len(stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "print(df_merged['cleaned_text'].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Step 3: 删除停用词，加入类型检查\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):  \n",
    "    \n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        # 删除停用词\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        \n",
    "        return ' '.join(filtered_words)\n",
    "    else:\n",
    "        # 如果不是字符串类型，返回原始值\n",
    "        return text\n",
    "\n",
    "# 应用到 'cleaned_text' 列\n",
    "df_merged['cleaned_text'] = df_merged['cleaned_text'].apply(remove_stopwords)\n",
    "\n",
    "# 查看处理后的数据\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text\n",
      "0  nice formula smooth application pooling shink ...\n",
      "1                        great product came expected\n",
      "2                year old mothers favorite fragrance\n",
      "3                                              worth\n",
      "4  battery life product always poor get one use l...\n"
     ]
    }
   ],
   "source": [
    "print(df_merged[['cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Nice formula, smooth application with no pooli...   \n",
      "1             Great product. It came when  expected.   \n",
      "2        My 90 year old mother's favorite fragrance.   \n",
      "3                                  Just not worth it   \n",
      "4  The Battery life on this product has always be...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  nice formula smooth application pooling shink ...  \n",
      "1                        great product came expected  \n",
      "2                 year old mother favorite fragrance  \n",
      "3                                              worth  \n",
      "4  battery life product always poor get one use l...  \n"
     ]
    }
   ],
   "source": [
    "# Step 4: 词形还原\n",
    "def lemmatize_text(text):\n",
    "    # 对文本进行标记化\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # 对每个词进行词形还原\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# 应用词形还原到 'cleaned_text' 列\n",
    "df_merged['cleaned_text'] = df_merged['cleaned_text'].apply(lemmatize_text)\n",
    "\n",
    "# 查看处理后的数据\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.drop(columns=['timestamp', 'verified_purchase'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating                                       title  \\\n",
      "0     5.0                                  Neon pink.   \n",
      "1     5.0                             Godd experience   \n",
      "2     5.0                Love This..made my mom's day   \n",
      "3     1.0                                      Review   \n",
      "4     5.0  Battery will not stay charged (Rescinding)   \n",
      "\n",
      "                                                text        task  \\\n",
      "0  Nice formula, smooth application with no pooli...  All_Beauty   \n",
      "1             Great product. It came when  expected.  All_Beauty   \n",
      "2        My 90 year old mother's favorite fragrance.  All_Beauty   \n",
      "3                                  Just not worth it  All_Beauty   \n",
      "4  The Battery life on this product has always be...  All_Beauty   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  nice formula smooth application pooling shink ...  \n",
      "1                        great product came expected  \n",
      "2                 year old mother favorite fragrance  \n",
      "3                                              worth  \n",
      "4  battery life product always poor get one use l...  \n"
     ]
    }
   ],
   "source": [
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371\n"
     ]
    }
   ],
   "source": [
    "print((df_merged['cleaned_text'] == \"\").sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating           0\n",
      "title           23\n",
      "text            27\n",
      "task             0\n",
      "cleaned_text     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_merged.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_merged.dropna(subset=['title', 'text', 'cleaned_text'])\n",
    "df_cleaned = df_cleaned[(df_cleaned['cleaned_text'] != \"\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(\"cleaned_amazon_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
