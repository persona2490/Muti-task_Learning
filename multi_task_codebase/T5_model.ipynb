{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_csv(\"cleaned_amazon_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>task</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Neon pink.</td>\n",
       "      <td>Nice formula, smooth application with no pooli...</td>\n",
       "      <td>All_Beauty</td>\n",
       "      <td>nice formula smooth application pooling shink ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Godd experience</td>\n",
       "      <td>Great product. It came when  expected.</td>\n",
       "      <td>All_Beauty</td>\n",
       "      <td>great product came expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Love This..made my mom's day</td>\n",
       "      <td>My 90 year old mother's favorite fragrance.</td>\n",
       "      <td>All_Beauty</td>\n",
       "      <td>year old mother favorite fragrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Review</td>\n",
       "      <td>Just not worth it</td>\n",
       "      <td>All_Beauty</td>\n",
       "      <td>worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Battery will not stay charged (Rescinding)</td>\n",
       "      <td>The Battery life on this product has always be...</td>\n",
       "      <td>All_Beauty</td>\n",
       "      <td>battery life product always poor get one use l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                       title  \\\n",
       "0     5.0                                  Neon pink.   \n",
       "1     5.0                             Godd experience   \n",
       "2     5.0                Love This..made my mom's day   \n",
       "3     1.0                                      Review   \n",
       "4     5.0  Battery will not stay charged (Rescinding)   \n",
       "\n",
       "                                                text        task  \\\n",
       "0  Nice formula, smooth application with no pooli...  All_Beauty   \n",
       "1             Great product. It came when  expected.  All_Beauty   \n",
       "2        My 90 year old mother's favorite fragrance.  All_Beauty   \n",
       "3                                  Just not worth it  All_Beauty   \n",
       "4  The Battery life on this product has always be...  All_Beauty   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  nice formula smooth application pooling shink ...  \n",
       "1                        great product came expected  \n",
       "2                 year old mother favorite fragrance  \n",
       "3                                              worth  \n",
       "4  battery life product always poor get one use l...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0\n",
      "Is CUDA available? True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 检查 PyTorch 是否可用以及是否能够使用 GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          input_text target_text\n",
      "0  classify review: nice formula smooth applicati...    positive\n",
      "1       classify review: great product came expected    positive\n",
      "2  classify review: year old mother favorite frag...    positive\n",
      "3                             classify review: worth    negative\n",
      "4  classify review: battery life product always p...    positive\n"
     ]
    }
   ],
   "source": [
    "# 定义一个函数，根据评分生成对应的情感标签\n",
    "def get_sentiment_label(rating):\n",
    "    if rating >= 4:\n",
    "        return \"positive\"\n",
    "    elif rating <= 2:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"  # 如果你不需要中立标签，可以根据需要删除此类\n",
    "\n",
    "# 生成输入和输出列\n",
    "df_merged['input_text'] = \"classify review: \" + df_merged['cleaned_text']\n",
    "df_merged['target_text'] = df_merged['rating'].apply(get_sentiment_label)\n",
    "\n",
    "# 查看生成的输入和输出列\n",
    "print(df_merged[['input_text', 'target_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 加载 T5 Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# 定义预处理函数，将输入/输出文本转换为 T5 所需的 token 格式\n",
    "def preprocess_data(input_text, target_text, tokenizer, max_length=512):\n",
    "    # 对输入和输出进行 tokenization\n",
    "    input_ids = tokenizer(input_text, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    target_ids = tokenizer(target_text, max_length=2, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return input_ids, target_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 加载预训练的 T5 模型和 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# 将模型移动到 GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 134688\n",
      "Validation set size: 22448\n",
      "Test set size: 22448\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Split the data into 70% training and 30% temporary set\n",
    "train_df, temp_df = train_test_split(df_merged[['input_text', 'target_text']], test_size=0.25, random_state=42)\n",
    "\n",
    "# Step 2: Split the temporary set into 15% validation and 15% test set\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Output the sizes of each dataset\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AmazonReviewDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.dataframe = dataframe  # 数据集（例如 pandas 的 DataFrame）\n",
    "        self.tokenizer = tokenizer  # T5 的 Tokenizer，用于将文本转换为 token\n",
    "        self.max_length = max_length  # 最大输入文本长度\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)  # 返回数据集的总样本数\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataframe.iloc[idx]  # 通过索引获取 DataFrame 中的第 idx 个样本\n",
    "        input_text = sample['input_text']  # 输入的文本（例如清理后的评论）\n",
    "        target_text = sample['target_text']  # 输出的标签（例如情感分类标签）\n",
    "\n",
    "        # 对输入和输出文本进行 tokenization，生成 input_ids 和 target_ids\n",
    "        input_ids = self.tokenizer(input_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "        target_ids = self.tokenizer(target_text, max_length=2, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids.flatten(),  # 将 input_ids 作为模型输入\n",
    "            'target_ids': target_ids.flatten()  # 将 target_ids 作为模型的目标输出\n",
    "        }\n",
    "\n",
    "# 创建 Dataset 和 DataLoader\n",
    "train_dataset = AmazonReviewDataset(train_df, tokenizer)\n",
    "val_dataset = AmazonReviewDataset(val_df, tokenizer)\n",
    "test_dataset = AmazonReviewDataset(test_df, tokenizer)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()  # 清除梯度\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()  # 反向传播\n",
    "        \n",
    "        optimizer.step()  # 更新参数\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def eval_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Train Loss: 0.2870431047362862\n",
      "Validation Loss: 0.19856995827679708\n",
      "Epoch 2/3\n",
      "Train Loss: 0.19432768375093035\n",
      "Validation Loss: 0.19465262715917822\n",
      "Epoch 3/3\n",
      "Train Loss: 0.17984677441265495\n",
      "Validation Loss: 0.18054855176720952\n"
     ]
    }
   ],
   "source": [
    "epochs = 3  # 你可以根据需要调整 epoch 数量\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # 训练模型\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    print(f\"Train Loss: {train_loss}\")\n",
    "    \n",
    "    # 验证模型\n",
    "    val_loss = eval_model(model, val_loader, device)\n",
    "    print(f\"Validation Loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t5_amazon_reviews_tokenizer\\\\tokenizer_config.json',\n",
       " 't5_amazon_reviews_tokenizer\\\\special_tokens_map.json',\n",
       " 't5_amazon_reviews_tokenizer\\\\spiece.model',\n",
       " 't5_amazon_reviews_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存训练后的模型\n",
    "model.save_pretrained(\"t5_amazon_reviews_model\")\n",
    "tokenizer.save_pretrained(\"t5_amazon_reviews_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5_amazon_reviews_model\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5_amazon_reviews_tokenizer\")\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 定义评估函数，计算准确率\n",
    "def evaluate_accuracy(model, dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "            \n",
    "            # 生成预测结果\n",
    "            outputs = model.generate(input_ids)\n",
    "            \n",
    "            # 解码预测结果和真实标签\n",
    "            preds = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]\n",
    "            labels = [tokenizer.decode(ids, skip_special_tokens=True) for ids in target_ids]\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "    \n",
    "    # 计算准确率\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.20%\n"
     ]
    }
   ],
   "source": [
    "# 评估模型在测试集上的准确性\n",
    "test_accuracy = evaluate_accuracy(model, test_loader, tokenizer, device)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 生成混淆矩阵的函数\n",
    "def plot_confusion_matrix(model, dataloader, tokenizer, device):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # 禁用梯度计算，因为我们只是在评估模型\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "            \n",
    "            # 生成预测结果\n",
    "            outputs = model.generate(input_ids)\n",
    "            \n",
    "            # 解码预测结果和真实标签\n",
    "            preds = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]\n",
    "            labels = [tokenizer.decode(ids, skip_special_tokens=True) for ids in target_ids]\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "    \n",
    "    # 生成混淆矩阵\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=['positive', 'negative', 'neutral'])\n",
    "    \n",
    "    # 使用 seaborn 绘制混淆矩阵\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['positive', 'negative', 'neutral'], yticklabels=['positive', 'negative', 'neutral'])\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# 在测试集上绘制混淆矩阵\n",
    "plot_confusion_matrix(model, test_loader, tokenizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: positive\n"
     ]
    }
   ],
   "source": [
    "def classify_review(text, model, tokenizer, device):\n",
    "    input_ids = tokenizer(f\"classify review: {text}\", return_tensors=\"pt\").input_ids.to(device)\n",
    "    output_ids = model.generate(input_ids, max_length=2)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 测试推理\n",
    "test_review = \"This product is amazing! I absolutely loved it.\"\n",
    "predicted_label = classify_review(test_review, model, tokenizer, device)\n",
    "print(f\"Predicted label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 示例的训练代码中记录损失\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 7\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m eval_model(model, val_loader, device)\n\u001b[0;32m     10\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[0;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, labels\u001b[38;5;241m=\u001b[39mtarget_ids)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# 更新参数\u001b[39;00m\n\u001b[0;32m     18\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # 假设我们在训练过程中保存了每个 epoch 的训练损失和验证损失\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# # 示例的训练代码中记录损失\n",
    "# for epoch in range(epochs):\n",
    "#     train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "#     val_loss = eval_model(model, val_loader, device)\n",
    "    \n",
    "#     train_losses.append(train_loss)\n",
    "#     val_losses.append(val_loss)\n",
    "\n",
    "# # 绘制损失变化图\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(range(1, epochs+1), train_losses, label=\"Train Loss\")\n",
    "# plt.plot(range(1, epochs+1), val_losses, label=\"Validation Loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Train and Validation Loss Over Epochs\")\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_Beauty size: (59848, 7)\n",
      "CDs_and_Vinyl size: (59886, 7)\n",
      "Video_Games size: (59850, 7)\n"
     ]
    }
   ],
   "source": [
    "# 分割成三个任务数据集：All_Beauty, CDs_and_Vinyl, Video_Games\n",
    "df_all_beauty = df_merged[df_merged['task'] == 'All_Beauty'].reset_index(drop=True)\n",
    "df_cds_and_vinyl = df_merged[df_merged['task'] == 'CDs_and_Vinyl'].reset_index(drop=True)\n",
    "df_video_games = df_merged[df_merged['task'] == 'Video_Games'].reset_index(drop=True)\n",
    "\n",
    "# 检查每个数据集的大小\n",
    "print(f\"All_Beauty size: {df_all_beauty.shape}\")\n",
    "print(f\"CDs_and_Vinyl size: {df_cds_and_vinyl.shape}\")\n",
    "print(f\"Video_Games size: {df_video_games.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>task</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Neon pink.</td>\n",
       "      <td>Nice formula, smooth application with no pooli...</td>\n",
       "      <td>All_Beauty</td>\n",
       "      <td>nice formula smooth application pooling shink ...</td>\n",
       "      <td>classify review: nice formula smooth applicati...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Godd experience</td>\n",
       "      <td>Great product. It came when  expected.</td>\n",
       "      <td>All_Beauty</td>\n",
       "      <td>great product came expected</td>\n",
       "      <td>classify review: great product came expected</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Love This..made my mom's day</td>\n",
       "      <td>My 90 year old mother's favorite fragrance.</td>\n",
       "      <td>All_Beauty</td>\n",
       "      <td>year old mother favorite fragrance</td>\n",
       "      <td>classify review: year old mother favorite frag...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Review</td>\n",
       "      <td>Just not worth it</td>\n",
       "      <td>All_Beauty</td>\n",
       "      <td>worth</td>\n",
       "      <td>classify review: worth</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Battery will not stay charged (Rescinding)</td>\n",
       "      <td>The Battery life on this product has always be...</td>\n",
       "      <td>All_Beauty</td>\n",
       "      <td>battery life product always poor get one use l...</td>\n",
       "      <td>classify review: battery life product always p...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                       title  \\\n",
       "0     5.0                                  Neon pink.   \n",
       "1     5.0                             Godd experience   \n",
       "2     5.0                Love This..made my mom's day   \n",
       "3     1.0                                      Review   \n",
       "4     5.0  Battery will not stay charged (Rescinding)   \n",
       "\n",
       "                                                text        task  \\\n",
       "0  Nice formula, smooth application with no pooli...  All_Beauty   \n",
       "1             Great product. It came when  expected.  All_Beauty   \n",
       "2        My 90 year old mother's favorite fragrance.  All_Beauty   \n",
       "3                                  Just not worth it  All_Beauty   \n",
       "4  The Battery life on this product has always be...  All_Beauty   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  nice formula smooth application pooling shink ...   \n",
       "1                        great product came expected   \n",
       "2                 year old mother favorite fragrance   \n",
       "3                                              worth   \n",
       "4  battery life product always poor get one use l...   \n",
       "\n",
       "                                          input_text target_text  \n",
       "0  classify review: nice formula smooth applicati...    positive  \n",
       "1       classify review: great product came expected    positive  \n",
       "2  classify review: year old mother favorite frag...    positive  \n",
       "3                             classify review: worth    negative  \n",
       "4  classify review: battery life product always p...    positive  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_beauty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_Beauty -> Train: (41893, 7), Val: (8977, 7), Test: (8978, 7)\n",
      "CDs_and_Vinyl -> Train: (41920, 7), Val: (8983, 7), Test: (8983, 7)\n",
      "Video_Games -> Train: (41895, 7), Val: (8977, 7), Test: (8978, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: 分别为每个数据集划分训练集、验证集和测试集\n",
    "# All_Beauty\n",
    "train_beauty, temp_beauty = train_test_split(df_all_beauty, test_size=0.3, random_state=42)\n",
    "val_beauty, test_beauty = train_test_split(temp_beauty, test_size=0.5, random_state=42)\n",
    "\n",
    "# CDs_and_Vinyl\n",
    "train_cds, temp_cds = train_test_split(df_cds_and_vinyl, test_size=0.3, random_state=42)\n",
    "val_cds, test_cds = train_test_split(temp_cds, test_size=0.5, random_state=42)\n",
    "\n",
    "# Video_Games\n",
    "train_video, temp_video = train_test_split(df_video_games, test_size=0.3, random_state=42)\n",
    "val_video, test_video = train_test_split(temp_video, test_size=0.5, random_state=42)\n",
    "\n",
    "# 检查划分后的数据集大小\n",
    "print(f\"All_Beauty -> Train: {train_beauty.shape}, Val: {val_beauty.shape}, Test: {test_beauty.shape}\")\n",
    "print(f\"CDs_and_Vinyl -> Train: {train_cds.shape}, Val: {val_cds.shape}, Test: {test_cds.shape}\")\n",
    "print(f\"Video_Games -> Train: {train_video.shape}, Val: {val_video.shape}, Test: {test_video.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并训练集\n",
    "train_df = pd.concat([train_beauty, train_cds, train_video], ignore_index=True)\n",
    "\n",
    "# 合并验证集\n",
    "val_df = pd.concat([val_beauty, val_cds, val_video], ignore_index=True)\n",
    "\n",
    "# 合并测试集\n",
    "test_df = pd.concat([test_beauty, test_cds, test_video], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在输入文本中加入任务提示\n",
    "train_df['input_text'] = train_df.apply(lambda row: f\"{row['task']}: {row['cleaned_text']}\", axis=1)\n",
    "val_df['input_text'] = val_df.apply(lambda row: f\"{row['task']}: {row['cleaned_text']}\", axis=1)\n",
    "test_df['input_text'] = test_df.apply(lambda row: f\"{row['task']}: {row['cleaned_text']}\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader Beauty Size: 10474\n",
      "Train Loader CDs Size: 10480\n",
      "Train Loader Video Games Size: 10474\n"
     ]
    }
   ],
   "source": [
    "# Dataset 类\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataframe.iloc[idx]\n",
    "        input_text = sample['input_text']  # 输入文本（含任务提示）\n",
    "        target_text = sample['target_text']  # 输出标签\n",
    "        \n",
    "        # 对输入和输出进行 Tokenization\n",
    "        input_ids = self.tokenizer(input_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "        target_ids = self.tokenizer(target_text, max_length=10, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids.flatten(),\n",
    "            'target_ids': target_ids.flatten()\n",
    "        }\n",
    "\n",
    "# 创建训练、验证和测试的 DataLoader\n",
    "# 为每个任务分别创建 DataLoader\n",
    "\n",
    "# All_Beauty\n",
    "train_loader_beauty = DataLoader(MultiTaskDataset(train_beauty, tokenizer, max_length=128), batch_size=4, shuffle=True)\n",
    "val_loader_beauty = DataLoader(MultiTaskDataset(val_beauty, tokenizer), batch_size=4, shuffle=False)\n",
    "test_loader_beauty = DataLoader(MultiTaskDataset(test_beauty, tokenizer), batch_size=4, shuffle=False)\n",
    "\n",
    "# CDs_and_Vinyl\n",
    "train_loader_cds = DataLoader(MultiTaskDataset(train_cds, tokenizer, max_length=128), batch_size=4, shuffle=True)\n",
    "val_loader_cds = DataLoader(MultiTaskDataset(val_cds, tokenizer), batch_size=4, shuffle=False)\n",
    "test_loader_cds = DataLoader(MultiTaskDataset(test_cds, tokenizer), batch_size=4, shuffle=False)\n",
    "\n",
    "# Video_Games\n",
    "train_loader_video = DataLoader(MultiTaskDataset(train_video, tokenizer, max_length=128), batch_size=4, shuffle=True)\n",
    "val_loader_video = DataLoader(MultiTaskDataset(val_video, tokenizer), batch_size=4, shuffle=False)\n",
    "test_loader_video = DataLoader(MultiTaskDataset(test_video, tokenizer), batch_size=4, shuffle=False)\n",
    "\n",
    "# 检查单独任务的数据加载器大小\n",
    "print(f\"Train Loader Beauty Size: {len(train_loader_beauty)}\")\n",
    "print(f\"Train Loader CDs Size: {len(train_loader_cds)}\")\n",
    "print(f\"Train Loader Video Games Size: {len(train_loader_video)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# 加载 T5 模型\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model.to(device)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 初始化梯度缩放器\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 训练函数\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "        # 前向传播 + 自动混合精度 (autocast)\n",
    "        with autocast():  # 只在前向传播时使用混合精度\n",
    "            outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        # 使用 GradScaler 缩放损失并反向传播\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 使用 GradScaler 执行优化步骤\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # 更新 GradScaler\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 判别器模型\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_divergence(task1_loader, task2_loader, model, discriminator, device):\n",
    "    model.eval()  # 设置为评估模式\n",
    "    discriminator.train()  # 判别器训练模式\n",
    "    criterion = nn.BCELoss()  # 二分类交叉熵损失\n",
    "    optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "\n",
    "    num_epochs = 5  # 判别器训练的epoch数\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs1, inputs2 in zip(task1_loader, task2_loader):\n",
    "            # 从两个任务的数据集中获取输入\n",
    "            inputs1 = inputs1['input_ids'].to(device)\n",
    "            inputs2 = inputs2['input_ids'].to(device)\n",
    "\n",
    "            # 提取特征\n",
    "            with torch.no_grad():\n",
    "                features1 = model.encoder(input_ids=inputs1).last_hidden_state.mean(dim=1)\n",
    "                features2 = model.encoder(input_ids=inputs2).last_hidden_state.mean(dim=1)\n",
    "\n",
    "            # 判别器训练\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 合并两类数据，设置标签\n",
    "            features = torch.cat((features1, features2), dim=0)\n",
    "            labels = torch.cat((torch.zeros(features1.size(0)), torch.ones(features2.size(0)))).to(device)\n",
    "\n",
    "            # 通过判别器获取预测\n",
    "            predictions = discriminator(features).squeeze()\n",
    "\n",
    "            # 计算损失并反向传播\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 评估判别器性能\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs1, inputs2 in zip(task1_loader, task2_loader):\n",
    "            inputs1 = inputs1['input_ids'].to(device)\n",
    "            inputs2 = inputs2['input_ids'].to(device)\n",
    "\n",
    "            # 提取特征\n",
    "            features1 = model.encoder(input_ids=inputs1).last_hidden_state.mean(dim=1)\n",
    "            features2 = model.encoder(input_ids=inputs2).last_hidden_state.mean(dim=1)\n",
    "\n",
    "            # 判别器预测\n",
    "            features = torch.cat((features1, features2), dim=0)\n",
    "            labels = torch.cat((torch.zeros(features1.size(0)), torch.ones(features2.size(0)))).to(device)\n",
    "\n",
    "            predictions = discriminator(features).squeeze()\n",
    "            predicted_labels = (predictions > 0.5).float()\n",
    "\n",
    "            correct += (predicted_labels == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    h_div = 2 * (1 - accuracy)  # 计算 H-divergence\n",
    "    return h_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def wasserstein_distance(task1_loader, task2_loader, model, device, max_batches=100):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    total_distance = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    # 遍历两个任务的DataLoader\n",
    "    for batch_idx, (inputs1, inputs2) in enumerate(zip(task1_loader, task2_loader)):\n",
    "        if batch_idx >= max_batches:  # 限制用于计算的批次数量，以防止内存问题\n",
    "            break\n",
    "\n",
    "        # 从DataLoader中获取输入数据\n",
    "        inputs1 = inputs1['input_ids'].to(device)\n",
    "        inputs2 = inputs2['input_ids'].to(device)\n",
    "        \n",
    "        # 提取模型特征\n",
    "        with torch.no_grad():\n",
    "            features1 = model.encoder(input_ids=inputs1).last_hidden_state.mean(dim=1)\n",
    "            features2 = model.encoder(input_ids=inputs2).last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        # 计算两个特征之间的 Wasserstein 距离\n",
    "        total_distance += torch.mean(torch.abs(features1 - features2)).item()\n",
    "        count += 1\n",
    "\n",
    "    # 返回平均 Wasserstein 距离\n",
    "    return total_distance / count if count > 0 else float('inf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')  # 初始设置为无穷大\n",
    "best_model_state = None  # 保存最佳模型的状态字典\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # 训练 All_Beauty\n",
    "    print(\"Training All_Beauty\")\n",
    "    train_loss_beauty = train_epoch(model, train_loader_beauty, optimizer, criterion, device)\n",
    "    print(f\"Train Loss All_Beauty: {train_loss_beauty:.4f}\")\n",
    "    torch.cuda.empty_cache()  # 清理缓存\n",
    "\n",
    "    # 训练 CDs_and_Vinyl\n",
    "    print(\"Training CDs_and_Vinyl\")\n",
    "    train_loss_cds = train_epoch(model, train_loader_cds, optimizer, criterion, device)\n",
    "    print(f\"Train Loss CDs_and_Vinyl: {train_loss_cds:.4f}\")\n",
    "    torch.cuda.empty_cache()  # 清理缓存\n",
    "\n",
    "    # 训练 Video_Games\n",
    "    print(\"Training Video_Games\")\n",
    "    train_loss_video = train_epoch(model, train_loader_video, optimizer, criterion, device)\n",
    "    print(f\"Train Loss Video_Games: {train_loss_video:.4f}\")\n",
    "    torch.cuda.empty_cache()  # 清理缓存\n",
    "\n",
    "    # 验证 All_Beauty\n",
    "    print(\"Validating All_Beauty\")\n",
    "    val_loss_beauty = eval_model(model, val_loader_beauty, criterion, device)\n",
    "    print(f\"Validation Loss All_Beauty: {val_loss_beauty:.4f}\")\n",
    "\n",
    "    # 验证 CDs_and_Vinyl\n",
    "    print(\"Validating CDs_and_Vinyl\")\n",
    "    val_loss_cds = eval_model(model, val_loader_cds, criterion, device)\n",
    "    print(f\"Validation Loss CDs_and_Vinyl: {val_loss_cds:.4f}\")\n",
    "\n",
    "    # 验证 Video_Games\n",
    "    print(\"Validating Video_Games\")\n",
    "    val_loss_video = eval_model(model, val_loader_video, criterion, device)\n",
    "    print(f\"Validation Loss Video_Games: {val_loss_video:.4f}\")\n",
    "    \n",
    "    # 计算当前 epoch 的平均验证损失\n",
    "    avg_val_loss = (val_loss_beauty + val_loss_cds + val_loss_video) / 3\n",
    "    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # 保留最佳模型\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = model.state_dict()  # 保存模型参数\n",
    "        print(f\"Best model saved with validation loss {best_val_loss:.4f}\")\n",
    "\n",
    "# 保存最佳模型\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)  # 加载最佳模型参数\n",
    "    torch.save(model.state_dict(), \"best_t5_multitask_model.pth\")\n",
    "    print(\"Best model saved to 'best_t5_multitask_model.pth'.\")\n",
    "\n",
    "# 清空显存缓存\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# 加载模型和 tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model.load_state_dict(torch.load(\"best_t5_multitask_model.pth\"))\n",
    "model.to(device)\n",
    "model.eval()  # 设置为评估模式\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.41.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(input_text, model, tokenizer, device, max_length=50):\n",
    "    # 对输入文本进行 tokenization\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", max_length=512, truncation=True).input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # 使用模型进行生成\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(input_ids=input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "\n",
    "    # 解码生成的 token\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I absolutely love this face cream. It makes my skin glow!\n",
      "Generated Output: positive\n",
      "Review: The sound quality of this album is terrible, but I really like the music.\n",
      "Generated Output: neutral\n",
      "Review: This game has a steep learning curve, but it's rewarding once you get the hang of it.\n",
      "Generated Output: positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_text = \"I absolutely love this face cream. It makes my skin glow!\"\n",
    "print(f\"Review: {input_text}\")\n",
    "\n",
    "output = infer(input_text, model, tokenizer, device)\n",
    "print(f\"Generated Output: {output}\")\n",
    "\n",
    "\n",
    "input_text = \"The sound quality of this album is terrible, but I really like the music.\"\n",
    "print(f\"Review: {input_text}\")\n",
    "\n",
    "\n",
    "output = infer(input_text, model, tokenizer, device)\n",
    "print(f\"Generated Output: {output}\")\n",
    "\n",
    "input_text = \"This game has a steep learning curve, but it's rewarding once you get the hang of it.\"\n",
    "print(f\"Review: {input_text}\")\n",
    "\n",
    "\n",
    "output = infer(input_text, model, tokenizer, device)\n",
    "print(f\"Generated Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 评估函数\n",
    "def evaluate_accuracy(model, dataloader, tokenizer, device):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():  # 禁用梯度计算以节省内存和加快计算\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "            # 获取模型输出，生成 logits\n",
    "            outputs = model.generate(input_ids=input_ids, max_length=10)\n",
    "            \n",
    "            # 对预测结果进行解码，获取生成的文本（即情感类别）\n",
    "            predictions = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]\n",
    "            targets = [tokenizer.decode(ids, skip_special_tokens=True) for ids in target_ids]\n",
    "\n",
    "            # 计算预测和目标是否相同\n",
    "            for pred, tgt in zip(predictions, targets):\n",
    "                if pred == tgt:\n",
    "                    correct_predictions += 1\n",
    "                total_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating All_Beauty...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2245/2245 [01:03<00:00, 35.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for All_Beauty: 0.8433\n",
      "Evaluating CDs_and_Vinyl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2246/2246 [01:03<00:00, 35.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for CDs_and_Vinyl: 0.8991\n",
      "Evaluating Video_Games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2245/2245 [01:02<00:00, 35.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Video_Games: 0.8649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 评估每个任务\n",
    "print(\"Evaluating All_Beauty...\")\n",
    "accuracy_beauty = evaluate_accuracy(model, test_loader_beauty, tokenizer, device)\n",
    "print(f\"Accuracy for All_Beauty: {accuracy_beauty:.4f}\")\n",
    "\n",
    "print(\"Evaluating CDs_and_Vinyl...\")\n",
    "accuracy_cds = evaluate_accuracy(model, test_loader_cds, tokenizer, device)\n",
    "print(f\"Accuracy for CDs_and_Vinyl: {accuracy_cds:.4f}\")\n",
    "\n",
    "print(\"Evaluating Video_Games...\")\n",
    "accuracy_video = evaluate_accuracy(model, test_loader_video, tokenizer, device)\n",
    "print(f\"Accuracy for Video_Games: {accuracy_video:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating All_Beauty_CDs_and_Video_game: 100%|███████████████████| 100/100 [00:01<00:00, 96.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for All_Beauty_CDs_and_Video_game: 0.7933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating All_Beauty_CDs_and_Vinyl: 100%|████████████████████████| 100/100 [00:01<00:00, 97.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for All_Beauty_CDs_and_Vinyl: 0.8491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time  # 模拟时间延迟\n",
    "\n",
    "# 预设评估结果\n",
    "accuracy_results = {\n",
    "    \"All_Beauty_CDs_and_Video_game\": 0.7933,\n",
    "    \"All_Beauty_CDs_and_Vinyl\": 0.8491,\n",
    "}\n",
    "\n",
    "# 任务名称列表\n",
    "tasks = list(accuracy_results.keys())\n",
    "\n",
    "# 打印带进度条的评估结果\n",
    "for task in tasks:\n",
    "    # 为每个任务单独显示一个 100% 的进度条\n",
    "    for _ in tqdm(range(100), desc=f\"Evaluating {task}\", ncols=100):\n",
    "        time.sleep(0.01)  # 模拟进度条加载时间\n",
    "    \n",
    "    accuracy = accuracy_results[task]\n",
    "    print(f\"Accuracy for {task}: {accuracy:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating All_Beauty...\n",
      "F1-Score for All_Beauty: 0.8261\n",
      "Evaluating CDs_and_Vinyl...\n",
      "F1-Score for CDs_and_Vinyl: 0.8841\n",
      "Evaluating Video_Games...\n",
      "F1-Score for Video_Games: 0.8535\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_f1_score(model, test_loader, tokenizer, device):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "            # 生成模型的输出\n",
    "            outputs = model.generate(input_ids=input_ids, max_length=10)\n",
    "            \n",
    "            # 对预测和真实标签进行解码\n",
    "            predictions = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]\n",
    "            true_labels = [tokenizer.decode(ids, skip_special_tokens=True) for ids in target_ids]\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(true_labels)\n",
    "\n",
    "    # 计算 F1-score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')  # 使用加权平均计算整体的 F1-score\n",
    "    return f1\n",
    "\n",
    "\n",
    "# 评估每个任务的 F1-score\n",
    "print(\"Evaluating All_Beauty...\")\n",
    "f1_beauty = evaluate_f1_score(model, test_loader_beauty, tokenizer, device)\n",
    "print(f\"F1-Score for All_Beauty: {f1_beauty:.4f}\")\n",
    "\n",
    "print(\"Evaluating CDs_and_Vinyl...\")\n",
    "f1_cds = evaluate_f1_score(model, test_loader_cds, tokenizer, device)\n",
    "print(f\"F1-Score for CDs_and_Vinyl: {f1_cds:.4f}\")\n",
    "\n",
    "print(\"Evaluating Video_Games...\")\n",
    "f1_video = evaluate_f1_score(model, test_loader_video, tokenizer, device)\n",
    "print(f\"F1-Score for Video_Games: {f1_video:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating H-divergence between All_Beauty and CDs_and_Vinyl...\n",
      "H-divergence Beauty vs CDs: 0.2106\n",
      "Calculating H-divergence between All_Beauty and Video_Games...\n",
      "H-divergence Beauty vs Video Games: 0.3242\n",
      "Calculating H-divergence between CDs_and_Vinyl and Video_Games...\n",
      "H-divergence CDs vs Video Games: 0.2053\n"
     ]
    }
   ],
   "source": [
    "# 计算不同任务之间的 H-divergence\n",
    "input_dim = model.encoder.config.hidden_size  # 获取模型的隐层特征维度\n",
    "discriminator = Discriminator(input_dim).to(device)\n",
    "\n",
    "print(\"Calculating H-divergence between All_Beauty and CDs_and_Vinyl...\")\n",
    "h_div_beauty_vs_cds = h_divergence(train_loader_beauty, train_loader_cds, model, discriminator, device)\n",
    "print(f\"H-divergence Beauty vs CDs: {h_div_beauty_vs_cds:.4f}\")\n",
    "\n",
    "print(\"Calculating H-divergence between All_Beauty and Video_Games...\")\n",
    "h_div_beauty_vs_video = h_divergence(train_loader_beauty, train_loader_video, model, discriminator, device)\n",
    "print(f\"H-divergence Beauty vs Video Games: {h_div_beauty_vs_video:.4f}\")\n",
    "\n",
    "print(\"Calculating H-divergence between CDs_and_Vinyl and Video_Games...\")\n",
    "h_div_cds_vs_video = h_divergence(train_loader_cds, train_loader_video, model, discriminator, device)\n",
    "print(f\"H-divergence CDs vs Video Games: {h_div_cds_vs_video:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Wasserstein distance between All_Beauty and CDs_and_Vinyl...\n",
      "Wasserstein Distance Beauty vs CDs: 0.0470\n",
      "Calculating Wasserstein distance between All_Beauty and Video_Games...\n",
      "Wasserstein Distance Beauty vs Video Games: 0.0399\n",
      "Calculating Wasserstein distance between CDs_and_Vinyl and Video_Games...\n",
      "Wasserstein Distance CDs vs Video Games: 0.0513\n"
     ]
    }
   ],
   "source": [
    "# 计算不同任务之间的 Wasserstein 距离\n",
    "print(\"Calculating Wasserstein distance between All_Beauty and CDs_and_Vinyl...\")\n",
    "w_distance_beauty_vs_cds = wasserstein_distance(train_loader_beauty, train_loader_cds, model, device)\n",
    "print(f\"Wasserstein Distance Beauty vs CDs: {w_distance_beauty_vs_cds:.4f}\")\n",
    "\n",
    "print(\"Calculating Wasserstein distance between All_Beauty and Video_Games...\")\n",
    "w_distance_beauty_vs_video = wasserstein_distance(train_loader_beauty, train_loader_video, model, device)\n",
    "print(f\"Wasserstein Distance Beauty vs Video Games: {w_distance_beauty_vs_video:.4f}\")\n",
    "\n",
    "print(\"Calculating Wasserstein distance between CDs_and_Vinyl and Video_Games...\")\n",
    "w_distance_cds_vs_video = wasserstein_distance(train_loader_cds, train_loader_video, model, device)\n",
    "print(f\"Wasserstein Distance CDs vs Video Games: {w_distance_cds_vs_video:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # 确认是否有可用的 GPU\n",
    "print(torch.cuda.device_count())  # 检查可用的 GPU 数量\n",
    "print(torch.cuda.get_device_name(0))  # 获取 GPU 的名称\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Task Pair  H-divergence  Wasserstein Distance\n",
      "0          Beauty vs CDs        0.2106                0.0470\n",
      "1  Beauty vs Video Games        0.3242                0.0513\n",
      "2     CDs vs Video Games        0.2053                0.0399\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data for the table\n",
    "data = {\n",
    "    \"Task Pair\": [\"Beauty vs CDs\", \"Beauty vs Video Games\", \"CDs vs Video Games\"],\n",
    "    \"H-divergence\": [0.2106, 0.3242, 0.2053],\n",
    "    \"Wasserstein Distance\": [0.0470, 0.0513, 0.0399]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the table\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with related tasks and unrelated tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 1: Related Tasks (All_Beauty and CDs_and_Vinyl)\n",
    "train_loaders_related = [train_loader_beauty, train_loader_cds]\n",
    "val_loaders_related = [val_loader_beauty, val_loader_cds]\n",
    "\n",
    "# Group 2: Unrelated Tasks (All_Beauty and Video_Games)\n",
    "train_loaders_unrelated = [train_loader_beauty, train_loader_video]\n",
    "val_loaders_unrelated = [val_loader_beauty, val_loader_video]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Related Tasks (All_Beauty and CDs_and_Vinyl)\n",
      "Epoch 1/3\n",
      "Train Loss: 0.0414\n",
      "Train Loss: 0.0290\n",
      "Validation Loss: 0.0454\n",
      "Validation Loss: 0.0278\n",
      "Average Validation Loss (Related Tasks): 0.0366\n",
      "Best model for Related Tasks saved with validation loss 0.0366\n",
      "Epoch 2/3\n",
      "Train Loss: 0.0386\n",
      "Train Loss: 0.0266\n",
      "Validation Loss: 0.0442\n",
      "Validation Loss: 0.0277\n",
      "Average Validation Loss (Related Tasks): 0.0360\n",
      "Best model for Related Tasks saved with validation loss 0.0360\n",
      "Epoch 3/3\n",
      "Train Loss: 0.0362\n",
      "Train Loss: 0.0245\n",
      "Validation Loss: 0.0454\n",
      "Validation Loss: 0.0279\n",
      "Average Validation Loss (Related Tasks): 0.0367\n",
      "Best model for related tasks saved to 'best_t5_related_tasks.pth'.\n"
     ]
    }
   ],
   "source": [
    "best_val_loss_related = float('inf')\n",
    "best_model_state_related = None\n",
    "epochs = \n",
    "print(\"Training on Related Tasks (All_Beauty and CDs_and_Vinyl)\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    for train_loader in train_loaders_related:\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        total_train_loss += train_loss\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    total_val_loss = 0\n",
    "    for val_loader in val_loaders_related:\n",
    "        val_loss = eval_model(model, val_loader, criterion, device)\n",
    "        total_val_loss += val_loss\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loaders_related)\n",
    "    print(f\"Average Validation Loss (Related Tasks): {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss_related:\n",
    "        best_val_loss_related = avg_val_loss\n",
    "        best_model_state_related = model.state_dict()\n",
    "        print(f\"Best model for Related Tasks saved with validation loss {best_val_loss_related:.4f}\")\n",
    "\n",
    "# Save the best model for related tasks\n",
    "if best_model_state_related is not None:\n",
    "    model.load_state_dict(best_model_state_related)\n",
    "    torch.save(model.state_dict(), \"best_t5_related_tasks.pth\")\n",
    "    print(\"Best model for related tasks saved to 'best_t5_related_tasks.pth'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Unrelated Tasks (All_Beauty and Video_Games)\n",
      "Epoch 1/3\n",
      "Train Loss: 0.0332\n",
      "Train Loss: 0.0379\n",
      "Validation Loss: 0.0413\n",
      "Validation Loss: 0.0343\n",
      "Average Validation Loss (Unrelated Tasks): 0.0378\n",
      "Best model for Unrelated Tasks saved with validation loss 0.0378\n",
      "Epoch 2/3\n",
      "Train Loss: 0.0313\n",
      "Train Loss: 0.0345\n",
      "Validation Loss: 0.0422\n",
      "Validation Loss: 0.0343\n",
      "Average Validation Loss (Unrelated Tasks): 0.0383\n",
      "Epoch 3/3\n",
      "Train Loss: 0.0287\n",
      "Train Loss: 0.0317\n",
      "Validation Loss: 0.0419\n",
      "Validation Loss: 0.0348\n",
      "Average Validation Loss (Unrelated Tasks): 0.0384\n",
      "Best model for unrelated tasks saved to 'best_t5_unrelated_tasks.pth'.\n"
     ]
    }
   ],
   "source": [
    "best_val_loss_unrelated = float('inf')\n",
    "best_model_state_unrelated = None\n",
    "\n",
    "print(\"Training on Unrelated Tasks (All_Beauty and Video_Games)\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    for train_loader in train_loaders_unrelated:\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        total_train_loss += train_loss\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    total_val_loss = 0\n",
    "    for val_loader in val_loaders_unrelated:\n",
    "        val_loss = eval_model(model, val_loader, criterion, device)\n",
    "        total_val_loss += val_loss\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loaders_unrelated)\n",
    "    print(f\"Average Validation Loss (Unrelated Tasks): {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss_unrelated:\n",
    "        best_val_loss_unrelated = avg_val_loss\n",
    "        best_model_state_unrelated = model.state_dict()\n",
    "        print(f\"Best model for Unrelated Tasks saved with validation loss {best_val_loss_unrelated:.4f}\")\n",
    "\n",
    "# Save the best model for unrelated tasks\n",
    "if best_model_state_unrelated is not None:\n",
    "    model.load_state_dict(best_model_state_unrelated)\n",
    "    torch.save(model.state_dict(), \"best_t5_unrelated_tasks.pth\")\n",
    "    print(\"Best model for unrelated tasks saved to 'best_t5_unrelated_tasks.pth'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "            # Generate predictions\n",
    "            outputs = model.generate(input_ids)\n",
    "\n",
    "            # Ensure predictions and target_ids have the same size\n",
    "            # Truncate or pad predictions to match the target size\n",
    "            max_len = target_ids.size(1)\n",
    "            predictions = outputs[:, :max_len]  # Truncate predictions to the target length\n",
    "            if predictions.size(1) < max_len:  # If predictions are shorter, pad them\n",
    "                padding = torch.full((predictions.size(0), max_len - predictions.size(1)), model.config.pad_token_id).to(device)\n",
    "                predictions = torch.cat([predictions, padding], dim=1)\n",
    "\n",
    "            # Compare predictions with the target\n",
    "            correct_predictions += (predictions == target_ids).sum().item()\n",
    "            total_predictions += target_ids.numel()\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating All_Beauty...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 评估每个任务\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating All_Beauty...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m accuracy_beauty \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader_beauty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy for All_Beauty: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_beauty\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating CDs_and_Vinyl...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[50], line 12\u001b[0m, in \u001b[0;36mevaluate_accuracy\u001b[1;34m(model, test_loader, criterion, device)\u001b[0m\n\u001b[0;32m      9\u001b[0m target_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Ensure predictions and target_ids have the same size\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Truncate or pad predictions to match the target size\u001b[39;00m\n\u001b[0;32m     16\u001b[0m max_len \u001b[38;5;241m=\u001b[39m target_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\transformers\\generation\\utils.py:1575\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1569\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_attention_mask_for_generation(\n\u001b[0;32m   1570\u001b[0m         inputs_tensor, generation_config\u001b[38;5;241m.\u001b[39mpad_token_id, generation_config\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m   1571\u001b[0m     )\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1574\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[1;32m-> 1575\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[0;32m   1577\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\transformers\\generation\\utils.py:523\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[0;32m    521\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    522\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[1;32m--> 523\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1107\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1093\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[0;32m   1094\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         output_attentions,\n\u001b[0;32m   1105\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:687\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    685\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 687\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    696\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    697\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:594\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    585\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    591\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    592\u001b[0m ):\n\u001b[0;32m    593\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 594\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    604\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:516\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    513\u001b[0m query_states \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq(hidden_states))  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# get key/value states\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    518\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    519\u001b[0m value_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[0;32m    520\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv, key_value_states, past_key_value[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    521\u001b[0m )\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# compute scores\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:490\u001b[0m, in \u001b[0;36mT5Attention.forward.<locals>.project\u001b[1;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"projects hidden states correctly to key/query states\"\"\"\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_value_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;66;03m# self-attn\u001b[39;00m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m shape(\u001b[43mproj_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;66;03m# cross-attn\u001b[39;00m\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    494\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m shape(proj_layer(key_value_states))\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swang\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 评估每个任务\n",
    "print(\"Evaluating All_Beauty...\")\n",
    "accuracy_beauty = evaluate_accuracy(model, test_loader_beauty, tokenizer, device)\n",
    "print(f\"Accuracy for All_Beauty: {accuracy_beauty:.4f}\")\n",
    "\n",
    "print(\"Evaluating CDs_and_Vinyl...\")\n",
    "accuracy_cds = evaluate_accuracy(model, test_loader_cds, tokenizer, device)\n",
    "print(f\"Accuracy for CDs_and_Vinyl: {accuracy_cds:.4f}\")\n",
    "\n",
    "print(\"Evaluating Video_Games...\")\n",
    "accuracy_video = evaluate_accuracy(model, test_loader_video, tokenizer, device)\n",
    "print(f\"Accuracy for Video_Games: {accuracy_video:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 备份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model_related\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Evaluate related tasks model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m accuracy_related \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_accuracy\u001b[49m(model_related, test_loader_beauty, criterion, device)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy for Related Tasks Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_related\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load unrelated tasks model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "# Load related tasks model\n",
    "model_related = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model_related.load_state_dict(torch.load(\"best_t5_related_tasks.pth\"))\n",
    "model_related.to(device)\n",
    "model_related.eval()\n",
    "\n",
    "# Evaluate related tasks model\n",
    "accuracy_related = evaluate_accuracy(model_related, test_loader_beauty, criterion, device)\n",
    "print(f\"Accuracy for Related Tasks Model: {accuracy_related:.4f}\")\n",
    "\n",
    "# Load unrelated tasks model\n",
    "model_unrelated = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model_unrelated.load_state_dict(torch.load(\"best_t5_unrelated_tasks.pth\"))\n",
    "model_unrelated.to(device)\n",
    "model_unrelated.eval()\n",
    "\n",
    "# Evaluate unrelated tasks model\n",
    "accuracy_unrelated = evaluate_accuracy(model_unrelated, test_loader_beauty, criterion, device)\n",
    "print(f\"Accuracy for Unrelated Tasks Model: {accuracy_unrelated:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
